{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b90b44b-beb9-459f-98b5-1d3503dff0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67a3c8a-a787-40fb-bafc-3681e03f1473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2951,  2671, 11859,  1996,  6840, 23732,  2090,  3274,  2671,\n",
       "          1010,  6747,  1010,  1998,  5884, 11532,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 1: Padding and tokenizing input sequence\n",
    "'''\n",
    "#example text data to be tokenized and padded \n",
    "text = \"Data science defines the intersectionality between computer science, statistics, and domain expertise.\" \n",
    "\n",
    "#DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#tokenize\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "'''\n",
    "add_special_tokens: adds special tokens start of sequence and end of sequence, which is needed for DistilBERT\n",
    "return_tensors: returns pytorch tensors\n",
    "'''\n",
    "\n",
    "#print tokenized and padded input\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553ec0c9-1539-416f-90b4-ba80f62bac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 2: Implement DistilBERT encoding layer to obtain contextual embeddings for sequence\n",
    "'''\n",
    "#DistilBERT model\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens, attention_mask = (tokens != 0).float())\n",
    "    \n",
    "contextual_embeddings = outputs.last_hidden_state\n",
    "contextual_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f21754-1de6-41ae-a532-0c5ddb46fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextComplexityLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomTextComplexityLayer, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Output is a single value\n",
    "\n",
    "        # Sigmoid activation to constrain the output between 0 and 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through fully connected layers with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply sigmoid activation to get a value between 0 and 1\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f88cfea-2463-41ec-b3a1-457156a41b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining previous steps to define the architecture\n",
    "class TextComplexityScoringModel(nn.Module):\n",
    "    def __init__(self, distilbert_model_name, hidden_size):\n",
    "        super(TextComplexityScoringModel, self).__init__()\n",
    "\n",
    "        # DistilBERT encoding layer\n",
    "        self.distilbert = DistilBertModel.from_pretrained(distilbert_model_name)\n",
    "\n",
    "        # Custom text complexity scoring layer\n",
    "        self.custom_layer = CustomTextComplexityLayer(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        contextual_embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Custom text complexity scoring layer forward pass\n",
    "        complexity_score = self.custom_layer(contextual_embeddings)\n",
    "        \n",
    "        return complexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33158b2a-3ae1-4d9c-9ace-38f16180937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Define the text complexity scoring model\n",
    "class TextComplexityScoringModel(nn.Module):\n",
    "    def __init__(self, distilbert_model_name, hidden_size, finetune_encoder = True):\n",
    "        super(TextComplexityScoringModel, self).__init__()\n",
    "\n",
    "        self.finetune_encoder = finetune_encoder\n",
    "        self.distilbert = DistilBertModel.from_pretrained(distilbert_model_name)\n",
    "        self.custom_layer = CustomTextComplexityLayer(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT forward pass\n",
    "        if self.finetune_encoder:\n",
    "            outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        contextual_embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Custom text complexity scoring layer forward pass\n",
    "        complexity_score = self.custom_layer(contextual_embeddings)\n",
    "        \n",
    "        return complexity_score\n",
    "\n",
    "# Define the custom text complexity scoring layer\n",
    "class CustomTextComplexityLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomTextComplexityLayer, self).__init__()\n",
    "        \n",
    "        #connect layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Output is a single value, 2nd parameter\n",
    "\n",
    "        #sigmoid function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through fully connected layers with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply sigmoid activation to get a value between 0 and 1\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# # Example text data\n",
    "# text = \"Data science defines the intersectionality between computer science, statistics, and domain expertise.\"\n",
    "\n",
    "# DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# # Tokenize and prepare input\n",
    "# tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt') #parameters can be changed\n",
    "# attention_mask = (tokens != 0).float()  #attention mask\n",
    "\n",
    "# Initialize the text complexity scoring model\n",
    "model = TextComplexityScoringModel('distilbert-base-uncased', hidden_size=768)  # Hidden size matches DistilBERT\n",
    "\n",
    "# # Forward pass through the model to obtain complexity score\n",
    "# complexity_score = model(tokens, attention_mask)\n",
    "\n",
    "# # Print the complexity score\n",
    "# print(complexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497a12a-e9d0-46cb-bfee-ac3c0bf418cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = example_string[\"Text\"][2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebea40-c27d-42aa-85c8-811b3857b525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fields count as STEM. Some STEM definitions include the , such as , economics, and anthropology. Most sources, however, consider these separate categories. U.S. Immigration and Customs Enforcement maintains a , which includes the four basic subjects above, along with architecture, psychology, digital communication, and some pharmaceutical and social sciences. Notably, fields like , , and are excluded from this list. to STEM, only officially recognized by the U.S. government in 2019. Many also and should be considered STEM. ACT Inc. includes many health and medical fields in its , giving doctors, nurses, and dentists the designation of STEM professionals. What Is a STEM Major? A STEM major is any major in a recognized STEM field. Note that colleges may have different definitions of what areas of study constitute a STEM major. Most undergraduate STEM programs culminate in a , though others may lead to a bachelor of applied science, a bachelor of engineering, or a bachelor of architecture. STEM students commonly take many of the same courses in fields like biology, chemistry, calculus, statistics, and engineering, regardless of their major. STEM degrees tend to be some of the in terms of the amount of time students typically spend completing assignments and preparing for class each week. Examples of Popular STEM Majors: Check Circle Check Circle Check Circle Check Circle Check Circle Check Circle Why Is STEM Important? As society innovates and technology advances, the need for professionals who understand how these technologies work and who can propose practical solutions continues to grow. The U.S. Bureau of Labor Statistics (BLS) calls STEM careers \",\" emphasizing the importance of these unique industries. Today, STEM jobs are in high demand, and many are projected to stay in demand for several years. At the same time, STEM professionals are in short supply, which'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_50_words = ' '.join(example_string[-50:])\n",
    "first_50_words = ' '.join(example_string[:50])\n",
    "\n",
    "#middle words\n",
    "middle_start = max(0, len(example_string) // 2 - 150)\n",
    "middle_end = min(len(example_string), len(example_string) // 2 + 150)\n",
    "\n",
    "middle_300_words = ' '.join(example_string[middle_start:middle_end])\n",
    "\n",
    "middle_300_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc1429-dcc8-4239-aa3f-a40dd3da8009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"STEM jobs are in high demand but suffer from a lack of qualified candidates. STEM is necessary for growing the economy and staying globally competitive. You've likely heard the term STEM, but what does it stand for? STEM is an acronym for science, technology, engineering, and math. These four fields\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_50_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0722a5e-9f89-419f-9f11-a650126ab5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"up to management positions, while others to conduct research. The BLS has identified , though this is not an exhaustive list. The following table presents some of the most popular STEM careers, as well as each job's median salary and projected employment outlook. Salary & Job Outlook for Popular STEM\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_50_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6dbdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parts_of_text(content):\n",
    "    words = content.split()  # Splitting the content into words\n",
    "    \n",
    "    # First 50 words\n",
    "    x = \" \".join(words[:50])\n",
    "    \n",
    "    # Middle 300 words\n",
    "    middle_index = len(words) // 2  # Integer division to get middle index\n",
    "    y_start = max(0, middle_index - 150)  # Ensure index is not negative\n",
    "    y_end = middle_index + 150\n",
    "    y = \" \".join(words[y_start:y_end])\n",
    "    \n",
    "    # Last 50 words\n",
    "    z = \" \".join(words[-50:])\n",
    "    \n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_50_words, middle_300_words, last_50_words = get_parts_of_text(example_string) \n",
    "domain = example_string[\"domain\"][2]\n",
    "content_length = len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25222e00-637d-4329-bd53-7ea295dbea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = f\"\"\"\n",
    "General information:\n",
    "Domain of Content:{domain}\n",
    "Content Length:{content_length}\n",
    "\n",
    "Text Snippets:\n",
    "First 50 words of content: \n",
    "{first_50_words}\n",
    "Middle 300 words of content:\n",
    "{middle_300_words}\n",
    "Last 50 words of content:\n",
    "{last_50_words}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361ae5d6-5915-4da7-bceb-7101527c468e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\William Zhang\\searchengine\\backend\\nn.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Tokenize and prepare input\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(test_example, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m400\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#parameters can be changed\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m (tokens \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()  \u001b[39m#attention mask\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Initialize the text complexity scoring model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_example' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare input\n",
    "tokens = tokenizer.encode(test_example, add_special_tokens=True, max_length=400, truncation=True, padding='max_length', return_tensors='pt') #parameters can be changed\n",
    "attention_mask = (tokens != 0).float()  #attention mask\n",
    "\n",
    "# Initialize the text complexity scoring model\n",
    "model = TextComplexityScoringModel('distilbert-base-uncased', hidden_size=768)  # Hidden size matches DistilBERT\n",
    "\n",
    "# Forward pass through the model to obtain complexity score\n",
    "complexity_score = model(tokens, attention_mask)\n",
    "\n",
    "# Print the complexity score\n",
    "print(complexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b31d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fb90cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df1 = pd.read_csv(\"1_6thfilteredscores.csv\")\n",
    "small_df2 = pd.read_csv(\"ordered_part_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c99cc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat([small_df1, small_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0b7995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import FloatTensor\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e6ef606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0007163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "\n",
    "        text = row[\"Text\"]\n",
    "        domain = row[\"Domain\"]\n",
    "        label = row[\"GPTeval\"]\n",
    "\n",
    "        first_50_words, middle_300_words, last_50_words = get_parts_of_text(text)\n",
    "\n",
    "        first_50_tokens = tokenizer.encode(first_50_words, add_special_tokens=False, max_length=50, truncation=True)\n",
    "        middle_300_tokens = tokenizer.encode(middle_300_words, add_special_tokens=False, max_length=300, truncation=True)\n",
    "        last_50_tokens = tokenizer.encode(last_50_words, add_special_tokens=False, max_length=50, truncation=True)\n",
    "\n",
    "        domain_tokens = tokenizer.encode(f\"Domain of Content: {domain}\", add_special_tokens=False)\n",
    "        length_tokens = tokenizer.encode(f\"Content Length: {len(word_tokenize(text))}\", add_special_tokens=False)\n",
    "\n",
    "        all_tokens = ([tokenizer.cls_token_id] + \n",
    "                  domain_tokens + [tokenizer.sep_token_id] +\n",
    "                  length_tokens + [tokenizer.sep_token_id] +\n",
    "                  first_50_tokens + [tokenizer.sep_token_id] + \n",
    "                  middle_300_tokens + [tokenizer.sep_token_id] + \n",
    "                  last_50_tokens + [tokenizer.sep_token_id])\n",
    "        \n",
    "        max_len = 512\n",
    "        padding_len = max_len - len(all_tokens)\n",
    "        all_tokens = all_tokens + ([tokenizer.pad_token_id] * padding_len) \n",
    "        attention_mask = (tokens != 0).float()\n",
    "\n",
    "        inputs_tensor = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        labels_tensor =torch.FloatTensor([label])\n",
    "\n",
    "        return inputs_tensor, labels_tensor, attention_mask    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8acbe0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(big_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "val_dataset = CustomDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e62c50f9-be96-425e-a5d7-37e35a1f236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6881fa44",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[256, 1, 1, 512]' is invalid for input of size 32768",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\William Zhang\\searchengine\\backend\\nn.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels, attention_mask) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader, \u001b[39m0\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     inputs, labels, attention_mask \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device), attention_mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     predictions \u001b[39m=\u001b[39m model(inputs, attention_mask)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(predictions, labels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\William Zhang\\searchengine\\backend\\nn.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# DistilBERT forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinetune_encoder:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    584\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[0;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    590\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    360\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:221\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    219\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    220\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[0;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[0;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    226\u001b[0m weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[256, 1, 1, 512]' is invalid for input of size 32768"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels, attention_mask) in enumerate(train_loader, 0):\n",
    "        inputs, labels, attention_mask = inputs.to(device), labels.to(device), attention_mask.to(device)\n",
    "        predictions = model(inputs, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print average loss every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Step the scheduler based on the validation loss\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        print(f\"[{epoch + 1}, {i + 1}] loss: {val_loss / len(val_loader):.3f}\")     \n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
