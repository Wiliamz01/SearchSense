{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b90b44b-beb9-459f-98b5-1d3503dff0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d67a3c8a-a787-40fb-bafc-3681e03f1473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2951,  2671, 11859,  1996,  6840, 23732,  2090,  3274,  2671,\n",
       "          1010,  6747,  1010,  1998,  5884, 11532,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 1: Padding and tokenizing input sequence\n",
    "'''\n",
    "#example text data to be tokenized and padded \n",
    "text = \"Data science defines the intersectionality between computer science, statistics, and domain expertise.\" \n",
    "\n",
    "#DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#tokenize\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "'''\n",
    "add_special_tokens: adds special tokens start of sequence and end of sequence, which is needed for DistilBERT\n",
    "return_tensors: returns pytorch tensors\n",
    "'''\n",
    "\n",
    "#print tokenized and padded input\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "553ec0c9-1539-416f-90b4-ba80f62bac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 2: Implement DistilBERT encoding layer to obtain contextual embeddings for sequence\n",
    "'''\n",
    "#DistilBERT model\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens, attention_mask = (tokens != 0).float())\n",
    "    \n",
    "contextual_embeddings = outputs.last_hidden_state\n",
    "contextual_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55f21754-1de6-41ae-a532-0c5ddb46fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextComplexityLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomTextComplexityLayer, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Output is a single value\n",
    "\n",
    "        # Sigmoid activation to constrain the output between 0 and 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through fully connected layers with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply sigmoid activation to get a value between 0 and 1\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88cfea-2463-41ec-b3a1-457156a41b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining previous steps to define the architecture\n",
    "class TextComplexityScoringModel(nn.Module):\n",
    "    def __init__(self, distilbert_model_name, hidden_size):\n",
    "        super(TextComplexityScoringModel, self).__init__()\n",
    "\n",
    "        # DistilBERT encoding layer\n",
    "        self.distilbert = DistilBertModel.from_pretrained(distilbert_model_name)\n",
    "\n",
    "        # Custom text complexity scoring layer\n",
    "        self.custom_layer = CustomTextComplexityLayer(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        contextual_embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Custom text complexity scoring layer forward pass\n",
    "        complexity_score = self.custom_layer(contextual_embeddings)\n",
    "        \n",
    "        return complexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33158b2a-3ae1-4d9c-9ace-38f16180937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Define the text complexity scoring model\n",
    "class TextComplexityScoringModel(nn.Module):\n",
    "    def __init__(self, distilbert_model_name, hidden_size):\n",
    "        super(TextComplexityScoringModel, self).__init__()\n",
    "\n",
    "       \n",
    "        self.distilbert = DistilBertModel.from_pretrained(distilbert_model_name)\n",
    "        self.custom_layer = CustomTextComplexityLayer(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        contextual_embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Custom text complexity scoring layer forward pass\n",
    "        complexity_score = self.custom_layer(contextual_embeddings)\n",
    "        \n",
    "        return complexity_score\n",
    "\n",
    "# Define the custom text complexity scoring layer\n",
    "class CustomTextComplexityLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomTextComplexityLayer, self).__init__()\n",
    "        \n",
    "        #connect layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Output is a single value, 2nd parameter\n",
    "\n",
    "        #sigmoid function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through fully connected layers with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply sigmoid activation to get a value between 0 and 1\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example text data\n",
    "text = \"Data science defines the intersectionality between computer science, statistics, and domain expertise.\"\n",
    "\n",
    "# DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize and prepare input\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt') #parameters can be changed\n",
    "attention_mask = (tokens != 0).float()  #attention mask\n",
    "\n",
    "# Initialize the text complexity scoring model\n",
    "model = TextComplexityScoringModel('distilbert-base-uncased', hidden_size=768)  # Hidden size matches DistilBERT\n",
    "\n",
    "# Forward pass through the model to obtain complexity score\n",
    "complexity_score = model(tokens, attention_mask)\n",
    "\n",
    "# Print the complexity score\n",
    "print(complexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78b2e642-9810-417a-b2d3-053e987ce03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "example_string = pd.read_csv(\"some_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c497a12a-e9d0-46cb-bfee-ac3c0bf418cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = example_string[\"Text\"][2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3ebea40-c27d-42aa-85c8-811b3857b525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fields count as STEM. Some STEM definitions include the , such as , economics, and anthropology. Most sources, however, consider these separate categories. U.S. Immigration and Customs Enforcement maintains a , which includes the four basic subjects above, along with architecture, psychology, digital communication, and some pharmaceutical and social sciences. Notably, fields like , , and are excluded from this list. to STEM, only officially recognized by the U.S. government in 2019. Many also and should be considered STEM. ACT Inc. includes many health and medical fields in its , giving doctors, nurses, and dentists the designation of STEM professionals. What Is a STEM Major? A STEM major is any major in a recognized STEM field. Note that colleges may have different definitions of what areas of study constitute a STEM major. Most undergraduate STEM programs culminate in a , though others may lead to a bachelor of applied science, a bachelor of engineering, or a bachelor of architecture. STEM students commonly take many of the same courses in fields like biology, chemistry, calculus, statistics, and engineering, regardless of their major. STEM degrees tend to be some of the in terms of the amount of time students typically spend completing assignments and preparing for class each week. Examples of Popular STEM Majors: Check Circle Check Circle Check Circle Check Circle Check Circle Check Circle Why Is STEM Important? As society innovates and technology advances, the need for professionals who understand how these technologies work and who can propose practical solutions continues to grow. The U.S. Bureau of Labor Statistics (BLS) calls STEM careers \",\" emphasizing the importance of these unique industries. Today, STEM jobs are in high demand, and many are projected to stay in demand for several years. At the same time, STEM professionals are in short supply, which'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_50_words = ' '.join(example_string[-50:])\n",
    "first_50_words = ' '.join(example_string[:50])\n",
    "\n",
    "#middle words\n",
    "middle_start = max(0, len(example_string) // 2 - 150)\n",
    "middle_end = min(len(example_string), len(example_string) // 2 + 150)\n",
    "\n",
    "middle_300_words = ' '.join(example_string[middle_start:middle_end])\n",
    "\n",
    "middle_300_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63cc1429-dcc8-4239-aa3f-a40dd3da8009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"STEM jobs are in high demand but suffer from a lack of qualified candidates. STEM is necessary for growing the economy and staying globally competitive. You've likely heard the term STEM, but what does it stand for? STEM is an acronym for science, technology, engineering, and math. These four fields\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_50_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0722a5e-9f89-419f-9f11-a650126ab5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"up to management positions, while others to conduct research. The BLS has identified , though this is not an exhaustive list. The following table presents some of the most popular STEM careers, as well as each job's median salary and projected employment outlook. Salary & Job Outlook for Popular STEM\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_50_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25222e00-637d-4329-bd53-7ea295dbea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = first_50_words + middle_300_words + last_50_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "361ae5d6-5915-4da7-bceb-7101527c468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5135],\n",
      "         [0.4757],\n",
      "         [0.4970],\n",
      "         [0.5060],\n",
      "         [0.4983],\n",
      "         [0.5006],\n",
      "         [0.5242],\n",
      "         [0.4954],\n",
      "         [0.5154],\n",
      "         [0.5056],\n",
      "         [0.5309],\n",
      "         [0.5080],\n",
      "         [0.5118],\n",
      "         [0.4981],\n",
      "         [0.5021],\n",
      "         [0.5196],\n",
      "         [0.4784],\n",
      "         [0.5045],\n",
      "         [0.4920],\n",
      "         [0.5028],\n",
      "         [0.5062],\n",
      "         [0.5147],\n",
      "         [0.5317],\n",
      "         [0.4984],\n",
      "         [0.5144],\n",
      "         [0.4984],\n",
      "         [0.5125],\n",
      "         [0.5210],\n",
      "         [0.5059],\n",
      "         [0.5128],\n",
      "         [0.5124],\n",
      "         [0.5015],\n",
      "         [0.4689],\n",
      "         [0.4921],\n",
      "         [0.4982],\n",
      "         [0.4774],\n",
      "         [0.5049],\n",
      "         [0.4947],\n",
      "         [0.4945],\n",
      "         [0.5184],\n",
      "         [0.4901],\n",
      "         [0.4911],\n",
      "         [0.5006],\n",
      "         [0.5087],\n",
      "         [0.4869],\n",
      "         [0.5017],\n",
      "         [0.4830],\n",
      "         [0.4780],\n",
      "         [0.4998],\n",
      "         [0.5149],\n",
      "         [0.4728],\n",
      "         [0.5022],\n",
      "         [0.4774],\n",
      "         [0.5157],\n",
      "         [0.5110],\n",
      "         [0.4800],\n",
      "         [0.4795],\n",
      "         [0.5158],\n",
      "         [0.5041],\n",
      "         [0.4877],\n",
      "         [0.4903],\n",
      "         [0.4897],\n",
      "         [0.4892],\n",
      "         [0.4705],\n",
      "         [0.4736],\n",
      "         [0.5009],\n",
      "         [0.4889],\n",
      "         [0.4812],\n",
      "         [0.4808],\n",
      "         [0.4821],\n",
      "         [0.4892],\n",
      "         [0.4946],\n",
      "         [0.4750],\n",
      "         [0.4867],\n",
      "         [0.4985],\n",
      "         [0.5291],\n",
      "         [0.5147],\n",
      "         [0.4835],\n",
      "         [0.5197],\n",
      "         [0.5170],\n",
      "         [0.4960],\n",
      "         [0.4803],\n",
      "         [0.4910],\n",
      "         [0.4822],\n",
      "         [0.5108],\n",
      "         [0.4790],\n",
      "         [0.4927],\n",
      "         [0.4772],\n",
      "         [0.4793],\n",
      "         [0.5157],\n",
      "         [0.5139],\n",
      "         [0.5233],\n",
      "         [0.4992],\n",
      "         [0.5060],\n",
      "         [0.4435],\n",
      "         [0.4610],\n",
      "         [0.4565],\n",
      "         [0.4607],\n",
      "         [0.4945],\n",
      "         [0.4726],\n",
      "         [0.4800],\n",
      "         [0.4679],\n",
      "         [0.4741],\n",
      "         [0.4909],\n",
      "         [0.4932],\n",
      "         [0.4794],\n",
      "         [0.4727],\n",
      "         [0.4732],\n",
      "         [0.5001],\n",
      "         [0.4595],\n",
      "         [0.4912],\n",
      "         [0.5098],\n",
      "         [0.4813],\n",
      "         [0.5157],\n",
      "         [0.4785],\n",
      "         [0.4747],\n",
      "         [0.4653],\n",
      "         [0.5172],\n",
      "         [0.4786],\n",
      "         [0.4816],\n",
      "         [0.4884],\n",
      "         [0.4938],\n",
      "         [0.4825],\n",
      "         [0.4996],\n",
      "         [0.5153],\n",
      "         [0.4606],\n",
      "         [0.4835],\n",
      "         [0.4818],\n",
      "         [0.4753],\n",
      "         [0.5010],\n",
      "         [0.5051],\n",
      "         [0.4900],\n",
      "         [0.4805],\n",
      "         [0.4553],\n",
      "         [0.4765],\n",
      "         [0.4768],\n",
      "         [0.4881],\n",
      "         [0.4903],\n",
      "         [0.4845],\n",
      "         [0.4919],\n",
      "         [0.4824],\n",
      "         [0.4738],\n",
      "         [0.4691],\n",
      "         [0.4674],\n",
      "         [0.4894],\n",
      "         [0.4970],\n",
      "         [0.5359],\n",
      "         [0.4998],\n",
      "         [0.5127],\n",
      "         [0.5112],\n",
      "         [0.5233],\n",
      "         [0.5069],\n",
      "         [0.4958],\n",
      "         [0.4877],\n",
      "         [0.4837],\n",
      "         [0.4783],\n",
      "         [0.4859],\n",
      "         [0.4878],\n",
      "         [0.4750],\n",
      "         [0.4486],\n",
      "         [0.4804],\n",
      "         [0.5050],\n",
      "         [0.4915],\n",
      "         [0.4777],\n",
      "         [0.5026],\n",
      "         [0.4550],\n",
      "         [0.4905],\n",
      "         [0.5060],\n",
      "         [0.4809],\n",
      "         [0.4910],\n",
      "         [0.4835],\n",
      "         [0.4709],\n",
      "         [0.4831],\n",
      "         [0.4858],\n",
      "         [0.5013],\n",
      "         [0.5038],\n",
      "         [0.4750],\n",
      "         [0.5105],\n",
      "         [0.5112],\n",
      "         [0.4763],\n",
      "         [0.4957],\n",
      "         [0.4898],\n",
      "         [0.4704],\n",
      "         [0.4751],\n",
      "         [0.4744],\n",
      "         [0.4725],\n",
      "         [0.4737],\n",
      "         [0.5196],\n",
      "         [0.4861],\n",
      "         [0.4977],\n",
      "         [0.4876],\n",
      "         [0.4787],\n",
      "         [0.4609],\n",
      "         [0.5002],\n",
      "         [0.4809],\n",
      "         [0.4831],\n",
      "         [0.4610],\n",
      "         [0.4873],\n",
      "         [0.4873],\n",
      "         [0.4730],\n",
      "         [0.4702],\n",
      "         [0.5003],\n",
      "         [0.4711],\n",
      "         [0.4745],\n",
      "         [0.4824],\n",
      "         [0.5185],\n",
      "         [0.4848],\n",
      "         [0.4799],\n",
      "         [0.5035],\n",
      "         [0.4993],\n",
      "         [0.4887],\n",
      "         [0.4563],\n",
      "         [0.4893],\n",
      "         [0.4867],\n",
      "         [0.4746],\n",
      "         [0.4603],\n",
      "         [0.4773],\n",
      "         [0.4966],\n",
      "         [0.4930],\n",
      "         [0.4867],\n",
      "         [0.4732],\n",
      "         [0.4616],\n",
      "         [0.5154],\n",
      "         [0.4916],\n",
      "         [0.5031],\n",
      "         [0.4842],\n",
      "         [0.4847],\n",
      "         [0.5038],\n",
      "         [0.4920],\n",
      "         [0.4923],\n",
      "         [0.4774],\n",
      "         [0.4651],\n",
      "         [0.4819],\n",
      "         [0.4600],\n",
      "         [0.4802],\n",
      "         [0.4909],\n",
      "         [0.4836],\n",
      "         [0.5096],\n",
      "         [0.4799],\n",
      "         [0.4800],\n",
      "         [0.4874],\n",
      "         [0.5048],\n",
      "         [0.4967],\n",
      "         [0.4680],\n",
      "         [0.4652],\n",
      "         [0.4829],\n",
      "         [0.5029],\n",
      "         [0.4939],\n",
      "         [0.5140],\n",
      "         [0.4663],\n",
      "         [0.4658],\n",
      "         [0.4774],\n",
      "         [0.4984],\n",
      "         [0.4894],\n",
      "         [0.5232],\n",
      "         [0.4805],\n",
      "         [0.4831],\n",
      "         [0.5134],\n",
      "         [0.4815],\n",
      "         [0.5076],\n",
      "         [0.5013],\n",
      "         [0.4989],\n",
      "         [0.4650],\n",
      "         [0.4624],\n",
      "         [0.4547],\n",
      "         [0.4743],\n",
      "         [0.4771],\n",
      "         [0.5031],\n",
      "         [0.4737],\n",
      "         [0.5053],\n",
      "         [0.4753],\n",
      "         [0.4736],\n",
      "         [0.4750],\n",
      "         [0.5011],\n",
      "         [0.5105],\n",
      "         [0.4676],\n",
      "         [0.4868],\n",
      "         [0.4986],\n",
      "         [0.4536],\n",
      "         [0.4757],\n",
      "         [0.4584],\n",
      "         [0.4462],\n",
      "         [0.5223],\n",
      "         [0.4848],\n",
      "         [0.4912],\n",
      "         [0.5197],\n",
      "         [0.5311],\n",
      "         [0.4856],\n",
      "         [0.4756],\n",
      "         [0.4956],\n",
      "         [0.4914],\n",
      "         [0.4798],\n",
      "         [0.4767],\n",
      "         [0.4989],\n",
      "         [0.4960],\n",
      "         [0.4574],\n",
      "         [0.5022],\n",
      "         [0.4771],\n",
      "         [0.4903],\n",
      "         [0.5227],\n",
      "         [0.5038],\n",
      "         [0.4881],\n",
      "         [0.4660],\n",
      "         [0.4876],\n",
      "         [0.5169],\n",
      "         [0.5129],\n",
      "         [0.5085],\n",
      "         [0.4988],\n",
      "         [0.4945],\n",
      "         [0.5220],\n",
      "         [0.4851],\n",
      "         [0.5083],\n",
      "         [0.4918],\n",
      "         [0.4749],\n",
      "         [0.4823],\n",
      "         [0.4949],\n",
      "         [0.4718],\n",
      "         [0.4815],\n",
      "         [0.4677],\n",
      "         [0.4811],\n",
      "         [0.4664],\n",
      "         [0.4833],\n",
      "         [0.4671],\n",
      "         [0.4829],\n",
      "         [0.4653],\n",
      "         [0.4845],\n",
      "         [0.4641],\n",
      "         [0.4850],\n",
      "         [0.5016],\n",
      "         [0.4918],\n",
      "         [0.4676],\n",
      "         [0.4610],\n",
      "         [0.5114],\n",
      "         [0.4798],\n",
      "         [0.5127],\n",
      "         [0.4820],\n",
      "         [0.5127],\n",
      "         [0.5029],\n",
      "         [0.4882],\n",
      "         [0.5014],\n",
      "         [0.5045],\n",
      "         [0.5228],\n",
      "         [0.5294],\n",
      "         [0.5118],\n",
      "         [0.5338],\n",
      "         [0.4854],\n",
      "         [0.5003],\n",
      "         [0.5086],\n",
      "         [0.4797],\n",
      "         [0.4977],\n",
      "         [0.4960],\n",
      "         [0.5079],\n",
      "         [0.4872],\n",
      "         [0.4925],\n",
      "         [0.5085],\n",
      "         [0.5116],\n",
      "         [0.4912],\n",
      "         [0.4808],\n",
      "         [0.5036],\n",
      "         [0.5072],\n",
      "         [0.4947],\n",
      "         [0.5236],\n",
      "         [0.5113],\n",
      "         [0.5265],\n",
      "         [0.5167],\n",
      "         [0.5164],\n",
      "         [0.5193],\n",
      "         [0.5105],\n",
      "         [0.5166],\n",
      "         [0.4988],\n",
      "         [0.5197],\n",
      "         [0.4809],\n",
      "         [0.5131],\n",
      "         [0.4955],\n",
      "         [0.4939],\n",
      "         [0.4497],\n",
      "         [0.4773],\n",
      "         [0.4915],\n",
      "         [0.4768],\n",
      "         [0.4979],\n",
      "         [0.4873],\n",
      "         [0.5072],\n",
      "         [0.5243],\n",
      "         [0.4764],\n",
      "         [0.4995],\n",
      "         [0.5019],\n",
      "         [0.4611],\n",
      "         [0.4879],\n",
      "         [0.5220],\n",
      "         [0.5051],\n",
      "         [0.5033],\n",
      "         [0.4777],\n",
      "         [0.4945],\n",
      "         [0.5122],\n",
      "         [0.4984],\n",
      "         [0.4899],\n",
      "         [0.5091],\n",
      "         [0.5088],\n",
      "         [0.4987],\n",
      "         [0.5159]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare input\n",
    "tokens = tokenizer.encode(test_example, add_special_tokens=True, max_length=400, truncation=True, padding='max_length', return_tensors='pt') #parameters can be changed\n",
    "attention_mask = (tokens != 0).float()  #attention mask\n",
    "\n",
    "# Initialize the text complexity scoring model\n",
    "model = TextComplexityScoringModel('distilbert-base-uncased', hidden_size=768)  # Hidden size matches DistilBERT\n",
    "\n",
    "# Forward pass through the model to obtain complexity score\n",
    "complexity_score = model(tokens, attention_mask)\n",
    "\n",
    "# Print the complexity score\n",
    "print(complexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c50f9-be96-425e-a5d7-37e35a1f236e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
