{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33158b2a-3ae1-4d9c-9ace-38f16180937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Define the text complexity scoring model\n",
    "class TextComplexityScoringModel(nn.Module):\n",
    "    def __init__(self, distilbert_model_name, hidden_size, finetune_encoder = True):\n",
    "        super(TextComplexityScoringModel, self).__init__()\n",
    "\n",
    "        self.finetune_encoder = finetune_encoder\n",
    "        self.distilbert = DistilBertModel.from_pretrained(distilbert_model_name)\n",
    "        self.custom_layer = CustomTextComplexityLayer()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT forward pass\n",
    "        if self.finetune_encoder:\n",
    "            #outputs = self.distilbert(input_ids[:,0,:], attention_mask=attention_mask[:, 0, :], output_hidden_states=True)\n",
    "\n",
    "            outputs = self.distilbert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #outputs = self.distilbert(input_ids[:, 0, :], attention_mask=attention_mask[:, 0, :], output_hidden_states=True)\n",
    "\n",
    "                outputs = self.distilbert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "        contextual_embeddings = outputs.last_hidden_state[:, -1, :]\n",
    "\n",
    "        # Custom text complexity scoring layer forward pass\n",
    "        complexity_score = self.custom_layer(contextual_embeddings)\n",
    "        \n",
    "        return complexity_score\n",
    "\n",
    "class CustomTextComplexityLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomTextComplexityLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)  # Batch Normalization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)  # Batch Normalization\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# # Example text data\n",
    "# text = \"Data science defines the intersectionality between computer science, statistics, and domain expertise.\"\n",
    "\n",
    "# DistilBERT tokenizer\n",
    "\n",
    "\n",
    "# tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "# # Tokenize and prepare input\n",
    "# tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt') #parameters can be changed\n",
    "# attention_mask = (tokens != 0).float()  #attention mask\n",
    "\n",
    "# Initialize the text complexity scoring model\n",
    "model = TextComplexityScoringModel('distilbert-base-uncased', hidden_size= 768)  # Hidden size matches DistilBERT\n",
    "\n",
    "# # Forward pass through the model to obtain complexity score\n",
    "# complexity_score = model(tokens, attention_mask)\n",
    "\n",
    "# # Print the complexity score\n",
    "# print(complexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6dbdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parts_of_text(content):\n",
    "    words = content.split()  # Splitting the content into words\n",
    "    \n",
    "    # First 50 words\n",
    "    x = \" \".join(words[:50])\n",
    "    \n",
    "    # Middle 300 words\n",
    "    middle_index = len(words) // 2  # Integer division to get middle index\n",
    "    y_start = max(0, middle_index - 150)  # Ensure index is not negative\n",
    "    y_end = middle_index + 150\n",
    "    y = \" \".join(words[y_start:y_end])\n",
    "    \n",
    "    # Last 50 words\n",
    "    z = \" \".join(words[-50:])\n",
    "    \n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b31d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfa8319-2af5-498c-96d0-7908c2c7f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "#normalized values, reset index\n",
    "normalized_big_df = pd.read_csv(\"normalized_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb90cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df1 = pd.read_csv(\"1_6thfilteredscores.csv\")\n",
    "small_df2 = pd.read_csv(\"ordered_part_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99cc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat([small_df1, small_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c04973",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = big_df['GPTeval'] < 0\n",
    "big_df.loc[condition, 'GPTeval'] = (big_df.loc[condition, 'GPTeval'] + 1) / 2\n",
    "condition = big_df['GPTeval'] > 1\n",
    "big_df.loc[condition, 'GPTeval'] = big_df.loc[condition, 'GPTeval'] * 0.1\n",
    "\n",
    "# big_df.to_csv(\"big_df1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0b7995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import FloatTensor\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f2cd43-4a0a-4f33-a5b5-c85ed6e38633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use the first available GPU (index 0)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0007163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.max_len = 128\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "\n",
    "        text = row[\"Text\"]\n",
    "        domain = row[\"Domain\"]\n",
    "        label = row[\"GPTeval\"]\n",
    "\n",
    "        first_50_words, middle_300_words, last_50_words = get_parts_of_text(text)\n",
    "\n",
    "        test_example = f\"\"\"\n",
    "        Domain of Content:{domain}\n",
    "        Content Length:{len(text.split())}\n",
    "\n",
    "        Text Snippets:\n",
    "        First 50 words of content: \n",
    "        {first_50_words}\n",
    "        Middle 300 words of content:\n",
    "        {middle_300_words}\n",
    "        Last 50 words of content:\n",
    "        {last_50_words}\n",
    "        \"\"\"\n",
    "\n",
    "        text_inputs = self.tokenizer.encode_plus(\n",
    "            text=test_example,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = text_inputs[\"input_ids\"]\n",
    "        mask = text_inputs[\"attention_mask\"]\n",
    "\n",
    "        labels_tensor = torch.FloatTensor([label])\n",
    "\n",
    "        return {\n",
    "                    \"ids\": ids,\n",
    "                    \"masks\": mask,\n",
    "                    \"labels\": labels_tensor,\n",
    "                }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8acbe0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(normalized_big_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "val_dataset = CustomDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62c50f9-be96-425e-a5d7-37e35a1f236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.01, weight_decay=1e-4) #potentially use this\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f926aae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\William Zhang\\searchengine\\backend\\nn1.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m batch_masks \u001b[39m=\u001b[39m batch_data_dict[\u001b[39m\"\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m batch_labels \u001b[39m=\u001b[39m batch_data_dict[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(batch_ids, batch_masks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, batch_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\William Zhang\\searchengine\\backend\\nn1.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# DistilBERT forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinetune_encoder:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39m#outputs = self.distilbert(input_ids[:,0,:], attention_mask=attention_mask[:, 0, :], output_hidden_states=True)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m             \u001b[39m#outputs = self.distilbert(input_ids[:, 0, :], attention_mask=attention_mask[:, 0, :], output_hidden_states=True)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    584\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[0;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    590\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    360\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:198\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    180\u001b[0m     query: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    186\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m    187\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39m        query: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39m        seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     bs, q_length, dim \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msize()\n\u001b[0;32m    199\u001b[0m     k_length \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[0;32m    200\u001b[0m     \u001b[39m# assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[39m# assert key.size() == value.size()\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.step()\n",
    "    print(epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch_data_dict in enumerate(train_loader, 0):\n",
    "        batch_ids = batch_data_dict[\"ids\"].to(device)\n",
    "        batch_masks = batch_data_dict[\"masks\"].to(device)\n",
    "        batch_labels = batch_data_dict[\"labels\"].to(device)\n",
    "        predictions = model(batch_ids, batch_masks)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        if not torch.isnan(loss):\n",
    "            loss.backward()\n",
    "        print(model.distilbert.embeddings.word_embeddings.weight.grad)\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print average loss every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Step the scheduler based on the validation loss\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data_dict in val_loader:\n",
    "            batch_ids = batch_data_dict[\"ids\"].to(device)\n",
    "            batch_masks = batch_data_dict[\"masks\"].to(device)\n",
    "            batch_labels = batch_data_dict[\"labels\"].to(device)\n",
    "            outputs = model(batch_ids, batch_masks)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "        print(f\"[{epoch + 1}, {i + 1}] loss: {val_loss / len(val_loader):.3f}\")     \n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd76d4eb-b262-4bd9-9d5a-f0b45b2790d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(big_df['GPTeval'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f29042-e4e4-4f60-b681-e34448a12af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Text</th>\n",
       "      <th>GPTeval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>gyandhan</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.gyandhan.com/blogs/stem-education-...</td>\n",
       "      <td>STEM Education System: A Complete Guide | Gyan...</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>invent</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.invent.org/blog/trends-stem/value-...</td>\n",
       "      <td>The Benefits of STEM Education for Children Si...</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>blog.teachmint</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://blog.teachmint.com/stem-education/</td>\n",
       "      <td>What is STEM Education? All You Need to Know |...</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>liysf.org</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.liysf.org.uk/blog/what-is-stem-edu...</td>\n",
       "      <td>What Is Stem Education? A Beginner's Guide - L...</td>\n",
       "      <td>0.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>education.gov</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.education.gov.au/australian-curric...</td>\n",
       "      <td>Introductory material - What is STEM? - Depart...</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0          Domain  \\\n",
       "0           1.0           0        gyandhan   \n",
       "1           2.0           0          invent   \n",
       "2           4.0           0  blog.teachmint   \n",
       "3           5.0           0       liysf.org   \n",
       "4           6.0           0   education.gov   \n",
       "\n",
       "                                             Title  \\\n",
       "0  What Does STEM Stand for in Education & School?   \n",
       "1  What Does STEM Stand for in Education & School?   \n",
       "2  What Does STEM Stand for in Education & School?   \n",
       "3  What Does STEM Stand for in Education & School?   \n",
       "4  What Does STEM Stand for in Education & School?   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.gyandhan.com/blogs/stem-education-...   \n",
       "1  https://www.invent.org/blog/trends-stem/value-...   \n",
       "2         https://blog.teachmint.com/stem-education/   \n",
       "3  https://www.liysf.org.uk/blog/what-is-stem-edu...   \n",
       "4  https://www.education.gov.au/australian-curric...   \n",
       "\n",
       "                                                Text  GPTeval  \n",
       "0  STEM Education System: A Complete Guide | Gyan...    0.857  \n",
       "1  The Benefits of STEM Education for Children Si...    0.680  \n",
       "2  What is STEM Education? All You Need to Know |...    0.680  \n",
       "3  What Is Stem Education? A Beginner's Guide - L...    0.570  \n",
       "4  Introductory material - What is STEM? - Depart...    0.680  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1930b562-b2df-4802-a50f-28b400003228",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = big_df.dropna(subset=['GPTeval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a658b4-80e7-437e-8fc5-3bb45d57eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.loc[(big_df['GPTeval'] > 1) & (big_df['GPTeval'] < 10), 'GPTeval'] = big_df['GPTeval'] / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ecff64-9cb1-4f49-bb69-97b442d2e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Text</th>\n",
       "      <th>GPTeval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>gyandhan</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.gyandhan.com/blogs/stem-education-...</td>\n",
       "      <td>STEM Education System: A Complete Guide | Gyan...</td>\n",
       "      <td>0.8570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>invent</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.invent.org/blog/trends-stem/value-...</td>\n",
       "      <td>The Benefits of STEM Education for Children Si...</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>blog.teachmint</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://blog.teachmint.com/stem-education/</td>\n",
       "      <td>What is STEM Education? All You Need to Know |...</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>liysf.org</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.liysf.org.uk/blog/what-is-stem-edu...</td>\n",
       "      <td>What Is Stem Education? A Beginner's Guide - L...</td>\n",
       "      <td>0.5700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>education.gov</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.education.gov.au/australian-curric...</td>\n",
       "      <td>Introductory material - What is STEM? - Depart...</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>turbofuture</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://turbofuture.com/computers/What-are-the...</td>\n",
       "      <td>What Are the Main Functions of a CPU? - TurboF...</td>\n",
       "      <td>0.7520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>totalphase</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://www.totalphase.com/blog/2022/08/what-i...</td>\n",
       "      <td>What is a CPU and What Does it Do? - Total Pha...</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>learnlearn</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://learnlearn.uk/alevelcs/the-cpu/</td>\n",
       "      <td>The CPU - A Level Computer Science Navigation ...</td>\n",
       "      <td>0.8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>totalphase</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://www.totalphase.com/blog/2022/08/what-i...</td>\n",
       "      <td>What is a CPU and What Does it Do? - Total Pha...</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>learnpick</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://www.learnpick.in/questions/details/636...</td>\n",
       "      <td>What is the function of CPU in a computer syst...</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2056 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0          Domain  \\\n",
       "0              1.0           0        gyandhan   \n",
       "1              2.0           0          invent   \n",
       "2              4.0           0  blog.teachmint   \n",
       "3              5.0           0       liysf.org   \n",
       "4              6.0           0   education.gov   \n",
       "...            ...         ...             ...   \n",
       "1334           NaN           0     turbofuture   \n",
       "1335           NaN           0      totalphase   \n",
       "1336           NaN           0      learnlearn   \n",
       "1337           NaN           0      totalphase   \n",
       "1338           NaN           0       learnpick   \n",
       "\n",
       "                                                  Title  \\\n",
       "0       What Does STEM Stand for in Education & School?   \n",
       "1       What Does STEM Stand for in Education & School?   \n",
       "2       What Does STEM Stand for in Education & School?   \n",
       "3       What Does STEM Stand for in Education & School?   \n",
       "4       What Does STEM Stand for in Education & School?   \n",
       "...                                                 ...   \n",
       "1334  What Is The Purpose Of The Central Processing ...   \n",
       "1335  What Is The Purpose Of The Central Processing ...   \n",
       "1336  What Is The Purpose Of The Central Processing ...   \n",
       "1337  What Is The Purpose Of The Central Processing ...   \n",
       "1338  What Is The Purpose Of The Central Processing ...   \n",
       "\n",
       "                                                    URL  \\\n",
       "0     https://www.gyandhan.com/blogs/stem-education-...   \n",
       "1     https://www.invent.org/blog/trends-stem/value-...   \n",
       "2            https://blog.teachmint.com/stem-education/   \n",
       "3     https://www.liysf.org.uk/blog/what-is-stem-edu...   \n",
       "4     https://www.education.gov.au/australian-curric...   \n",
       "...                                                 ...   \n",
       "1334  https://turbofuture.com/computers/What-are-the...   \n",
       "1335  https://www.totalphase.com/blog/2022/08/what-i...   \n",
       "1336            https://learnlearn.uk/alevelcs/the-cpu/   \n",
       "1337  https://www.totalphase.com/blog/2022/08/what-i...   \n",
       "1338  https://www.learnpick.in/questions/details/636...   \n",
       "\n",
       "                                                   Text  GPTeval  \n",
       "0     STEM Education System: A Complete Guide | Gyan...   0.8570  \n",
       "1     The Benefits of STEM Education for Children Si...   0.6800  \n",
       "2     What is STEM Education? All You Need to Know |...   0.6800  \n",
       "3     What Is Stem Education? A Beginner's Guide - L...   0.5700  \n",
       "4     Introductory material - What is STEM? - Depart...   0.6800  \n",
       "...                                                 ...      ...  \n",
       "1334  What Are the Main Functions of a CPU? - TurboF...   0.7520  \n",
       "1335  What is a CPU and What Does it Do? - Total Pha...   0.8750  \n",
       "1336  The CPU - A Level Computer Science Navigation ...   0.8370  \n",
       "1337  What is a CPU and What Does it Do? - Total Pha...   0.8750  \n",
       "1338  What is the function of CPU in a computer syst...   0.1875  \n",
       "\n",
       "[2056 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f8fb7d-3d2e-4833-b45e-2ce1982ab83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your original DataFrame\n",
    "normalized_big_df = big_df.copy()  # Make a copy of the original DataFrame\n",
    "\n",
    "# Normalize the \"GPTeval\" column in normalized_big_df\n",
    "min_value = normalized_big_df['GPTeval'].min()\n",
    "max_value = normalized_big_df['GPTeval'].max()\n",
    "normalized_big_df['GPTeval'] = (normalized_big_df['GPTeval'] - min_value) / (max_value - min_value)\n",
    "\n",
    "# Drop the \"Unnamed: 0.1\" and \"Unnamed: 0\" columns\n",
    "normalized_big_df = normalized_big_df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "\n",
    "# Reset the index to start from 0\n",
    "normalized_big_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "normalized_big_df.to_csv(\"normalized_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a2f6fd-4e51-4bdb-9422-5a3183b5fe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Text</th>\n",
       "      <th>GPTeval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gyandhan</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.gyandhan.com/blogs/stem-education-...</td>\n",
       "      <td>STEM Education System: A Complete Guide | Gyan...</td>\n",
       "      <td>0.8570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invent</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.invent.org/blog/trends-stem/value-...</td>\n",
       "      <td>The Benefits of STEM Education for Children Si...</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blog.teachmint</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://blog.teachmint.com/stem-education/</td>\n",
       "      <td>What is STEM Education? All You Need to Know |...</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>liysf.org</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.liysf.org.uk/blog/what-is-stem-edu...</td>\n",
       "      <td>What Is Stem Education? A Beginner's Guide - L...</td>\n",
       "      <td>0.5700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>education.gov</td>\n",
       "      <td>What Does STEM Stand for in Education &amp; School?</td>\n",
       "      <td>https://www.education.gov.au/australian-curric...</td>\n",
       "      <td>Introductory material - What is STEM? - Depart...</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>turbofuture</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://turbofuture.com/computers/What-are-the...</td>\n",
       "      <td>What Are the Main Functions of a CPU? - TurboF...</td>\n",
       "      <td>0.7520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>totalphase</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://www.totalphase.com/blog/2022/08/what-i...</td>\n",
       "      <td>What is a CPU and What Does it Do? - Total Pha...</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>learnlearn</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://learnlearn.uk/alevelcs/the-cpu/</td>\n",
       "      <td>The CPU - A Level Computer Science Navigation ...</td>\n",
       "      <td>0.8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>totalphase</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://www.totalphase.com/blog/2022/08/what-i...</td>\n",
       "      <td>What is a CPU and What Does it Do? - Total Pha...</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>learnpick</td>\n",
       "      <td>What Is The Purpose Of The Central Processing ...</td>\n",
       "      <td>https://www.learnpick.in/questions/details/636...</td>\n",
       "      <td>What is the function of CPU in a computer syst...</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2056 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Domain                                              Title  \\\n",
       "0           gyandhan    What Does STEM Stand for in Education & School?   \n",
       "1             invent    What Does STEM Stand for in Education & School?   \n",
       "2     blog.teachmint    What Does STEM Stand for in Education & School?   \n",
       "3          liysf.org    What Does STEM Stand for in Education & School?   \n",
       "4      education.gov    What Does STEM Stand for in Education & School?   \n",
       "...              ...                                                ...   \n",
       "2051     turbofuture  What Is The Purpose Of The Central Processing ...   \n",
       "2052      totalphase  What Is The Purpose Of The Central Processing ...   \n",
       "2053      learnlearn  What Is The Purpose Of The Central Processing ...   \n",
       "2054      totalphase  What Is The Purpose Of The Central Processing ...   \n",
       "2055       learnpick  What Is The Purpose Of The Central Processing ...   \n",
       "\n",
       "                                                    URL  \\\n",
       "0     https://www.gyandhan.com/blogs/stem-education-...   \n",
       "1     https://www.invent.org/blog/trends-stem/value-...   \n",
       "2            https://blog.teachmint.com/stem-education/   \n",
       "3     https://www.liysf.org.uk/blog/what-is-stem-edu...   \n",
       "4     https://www.education.gov.au/australian-curric...   \n",
       "...                                                 ...   \n",
       "2051  https://turbofuture.com/computers/What-are-the...   \n",
       "2052  https://www.totalphase.com/blog/2022/08/what-i...   \n",
       "2053            https://learnlearn.uk/alevelcs/the-cpu/   \n",
       "2054  https://www.totalphase.com/blog/2022/08/what-i...   \n",
       "2055  https://www.learnpick.in/questions/details/636...   \n",
       "\n",
       "                                                   Text  GPTeval  \n",
       "0     STEM Education System: A Complete Guide | Gyan...   0.8570  \n",
       "1     The Benefits of STEM Education for Children Si...   0.6800  \n",
       "2     What is STEM Education? All You Need to Know |...   0.6800  \n",
       "3     What Is Stem Education? A Beginner's Guide - L...   0.5700  \n",
       "4     Introductory material - What is STEM? - Depart...   0.6800  \n",
       "...                                                 ...      ...  \n",
       "2051  What Are the Main Functions of a CPU? - TurboF...   0.7520  \n",
       "2052  What is a CPU and What Does it Do? - Total Pha...   0.8750  \n",
       "2053  The CPU - A Level Computer Science Navigation ...   0.8370  \n",
       "2054  What is a CPU and What Does it Do? - Total Pha...   0.8750  \n",
       "2055  What is the function of CPU in a computer syst...   0.1875  \n",
       "\n",
       "[2056 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38427b66-d0bb-4505-a253-22144274dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275391a2-5eef-41c3-80cc-74ea498b4327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_big_df[\"GPTeval\"][130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8a6b4a0-2a88-4087-898f-009a41875360",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_big_df.to_csv(\"normalized_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c155ead-1056-4375-ad04-9133931a5810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd28af-cc43-44cd-b0c2-f72bd850c5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b24c5-6d46-4f55-8016-ef8e729ac18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24099a-8578-4b3c-82b0-91199a1d9fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ecf774-1929-454d-aa46-0dd79bd584df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6e313-3b5a-4a38-84fe-a744b05b7dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21d9f9-54f1-4c71-b328-75ceefa8202a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1df4b49-dcef-47e2-a484-2d60f16cb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72f6290-1c2a-461a-aca2-c933c6a99c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\William Zhang\\searchengine\\backend\\nn1.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X50sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(words[\u001b[39m-\u001b[39m\u001b[39m50\u001b[39m:])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X50sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x, y, z\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X50sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustomDataset\u001b[39;00m(Dataset):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X50sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,dataframe):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/William%20Zhang/searchengine/backend/nn1.ipynb#X50sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataframe \u001b[39m=\u001b[39m dataframe\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Define the text complexity scoring model\n",
    "class TextComplexityScoringModel(nn.Module):\n",
    "    def __init__(self, distilbert_model_name, hidden_size, finetune_encoder = True):\n",
    "        super(TextComplexityScoringModel, self).__init__()\n",
    "\n",
    "        self.finetune_encoder = finetune_encoder\n",
    "        self.distilbert = DistilBertModel.from_pretrained(distilbert_model_name)\n",
    "        self.custom_layer = CustomTextComplexityLayer()\n",
    "        \n",
    "        if self.finetune_encoder:\n",
    "            for param in self.distilbert.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.distilbert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT forward pass\n",
    "        if self.finetune_encoder:\n",
    "            #outputs = self.distilbert(input_ids[:,0,:], attention_mask=attention_mask[:, 0, :], output_hidden_states=True)\n",
    "  \n",
    "        \n",
    "            \n",
    "            outputs = self.distilbert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #outputs = self.distilbert(input_ids[:, 0, :], attention_mask=attention_mask[:, 0, :], output_hidden_states=True)\n",
    "       \n",
    "                outputs = self.distilbert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "                \n",
    "                \n",
    "        contextual_embeddings = outputs.last_hidden_state[:, -1, :]\n",
    "\n",
    "        # Custom text complexity scoring layer forward pass\n",
    "        complexity_score = self.custom_layer(contextual_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return complexity_score\n",
    "\n",
    "class CustomTextComplexityLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomTextComplexityLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)  # Batch Normalization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)  # Batch Normalization\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# # Example text data\n",
    "# text = \"Data science defines the intersectionality between computer science, statistics, and domain expertise.\"\n",
    "\n",
    "# DistilBERT tokenizer\n",
    "\n",
    "\n",
    "# tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "# # Tokenize and prepare input\n",
    "# tokens = tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt') #parameters can be changed\n",
    "# attention_mask = (tokens != 0).float()  #attention mask\n",
    "\n",
    "# Initialize the text complexity scoring model\n",
    "model = TextComplexityScoringModel('distilbert-base-uncased', hidden_size= 768)  # Hidden size matches DistilBERT\n",
    "\n",
    "# # Forward pass through the model to obtain complexity score\n",
    "# complexity_score = model(tokens, attention_mask)\n",
    "\n",
    "# # Print the complexity score\n",
    "# print(complexity_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_parts_of_text(content):\n",
    "    words = content.split()  # Splitting the content into words\n",
    "    \n",
    "    # First 50 words\n",
    "    x = \" \".join(words[:50])\n",
    "    \n",
    "    # Middle 300 words\n",
    "    middle_index = len(words) // 2  # Integer division to get middle index\n",
    "    y_start = max(0, middle_index - 150)  # Ensure index is not negative\n",
    "    y_end = middle_index + 150\n",
    "    y = \" \".join(words[y_start:y_end])\n",
    "    \n",
    "    # Last 50 words\n",
    "    z = \" \".join(words[-50:])\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.max_len = 128\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "\n",
    "        text = row[\"Text\"]\n",
    "        domain = row[\"Domain\"]\n",
    "        label = row[\"GPTeval\"]\n",
    "\n",
    "        first_50_words, middle_300_words, last_50_words = get_parts_of_text(text)\n",
    "\n",
    "        test_example = f\"\"\"\n",
    "        Domain of Content:{domain}\n",
    "        Content Length:{len(text.split())}\n",
    "\n",
    "        Text Snippets:\n",
    "        First 50 words of content: \n",
    "        {first_50_words}\n",
    "        Middle 300 words of content:\n",
    "        {middle_300_words}\n",
    "        Last 50 words of content:\n",
    "        {last_50_words}\n",
    "        \"\"\"\n",
    "\n",
    "        text_inputs = self.tokenizer.encode_plus(\n",
    "            text= test_example,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = text_inputs[\"input_ids\"]\n",
    "        mask = text_inputs[\"attention_mask\"]\n",
    "\n",
    "        labels_tensor = torch.FloatTensor([label])\n",
    "\n",
    "        return {\n",
    "                    \"ids\": ids,\n",
    "                    \"masks\": mask,\n",
    "                    \"labels\": labels_tensor,\n",
    "                }\n",
    "        \n",
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# #optimizer = optim.SGD(model.parameters(), lr = 0.01, weight_decay=1e-4) #potentially use this\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay=1e-4)\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     for i, batch_data_dict in enumerate(train_loader, 0):\n",
    "        \n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Get the inputs and move them to the device\n",
    "#         batch_ids = batch_data_dict[\"ids\"].squeeze(1).to(device)  # Remove dimension of size 1\n",
    "\n",
    "#         batch_masks = batch_data_dict[\"masks\"].squeeze(1).to(device)  # Remove dimension of size 1\n",
    "#         batch_labels = batch_data_dict[\"labels\"].to(device)\n",
    "        \n",
    "#         print(batch_ids.shape, batch_masks.shape, batch_labels.shape)  # Debug Step 1\n",
    "\n",
    "#         # Forward pass\n",
    "#         predictions = model(batch_ids, batch_masks)\n",
    "        \n",
    "#         print(predictions.shape, predictions)  # Debug Step 2\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = criterion(predictions, batch_labels)\n",
    "        \n",
    "#         print(loss.item())  # Debug Step 3\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         if not torch.isnan(loss):\n",
    "#             loss.backward()\n",
    "#             if any(param.grad is None for param in model.parameters()):\n",
    "#                 print(\"Gradients not updated for some parameters!\")\n",
    "            \n",
    "#             optimizer.step()\n",
    "\n",
    "#         print(model.distilbert.embeddings.word_embeddings.weight.grad)  # Debug Step 4\n",
    "        \n",
    "#         # Print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 100 == 99:  # Print every 100 mini-batches\n",
    "#             print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "#             running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190f37b-75b6-4178-8d3f-36e984659379",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, batch_data_dict in enumerate(train_loader, 0):\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get the inputs and move them to the device\n",
    "        batch_ids = batch_data_dict[\"ids\"].squeeze(1).to(device)  # Remove dimension of size 1\n",
    "\n",
    "        batch_masks = batch_data_dict[\"masks\"].squeeze(1).to(device)  # Remove dimension of size 1\n",
    "        batch_labels = batch_data_dict[\"labels\"].to(device)\n",
    "        \n",
    "        print(batch_ids.shape, batch_masks.shape, batch_labels.shape)  # Debug Step 1\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(batch_ids, batch_masks)\n",
    "        \n",
    "        print(predictions.shape, predictions)  # Debug Step 2\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        \n",
    "        print(loss.item())  # Debug Step 3\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        if not torch.isnan(loss):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(model.distilbert.embeddings.word_embeddings.weight.grad)  # Debug Step 4\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "27f2179a-b6e8-4d1c-bfda-2eda7ef863cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128]) torch.Size([32, 1, 128]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(batch_ids.shape, batch_masks.shape, batch_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a361dbb-9132-48cf-868b-6c39ab00cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SimpleTextComplexityModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SimpleTextComplexityModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "vocab_size = 5000  # Example vocab size\n",
    "embed_size = 100  # Example embedding size\n",
    "\n",
    "model = SimpleTextComplexityModel(vocab_size, embed_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example data\n",
    "texts = [[1, 2, 3], [2, 3, 4], [4, 5, 6]]  # Replace with actual tokenized texts\n",
    "labels = [0.2, 0.5, 0.8]  # Replace with actual complexity labels\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = CustomDataset(texts, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        texts = batch['text']\n",
    "        labels = batch['label'].view(-1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e21b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031a9634-b71f-4554-9fe8-5e179d7753f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_big_df = pd.read_csv(\"normalized_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1885323a-6dc1-41a1-80ac-4c4a2beaebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming normalized_big_df contains your data\n",
    "# Extract the texts and complexity labels\n",
    "texts = normalized_big_df['Text'].tolist()\n",
    "labels = normalized_big_df['GPTeval'].tolist()\n",
    "\n",
    "# # Initialize a list to store the transformed data\n",
    "# transformed_data = []\n",
    "\n",
    "# # Iterate through the texts and labels\n",
    "# for text, label in zip(texts, labels):\n",
    "#     # Split the text into tokens\n",
    "#     tokens = text.split()\n",
    "    \n",
    "#     # Calculate the lengths of the beginning, middle, and end sections\n",
    "#     total_tokens = len(tokens)\n",
    "#     beg_length = int(total_tokens * label)  # Length of the \"beginning\"\n",
    "#     mid_length = int(total_tokens * (1 - label) / 2)  # Length of the \"middle\"\n",
    "#     end_length = total_tokens - beg_length - mid_length  # Length of the \"end\"\n",
    "    \n",
    "#     # Create the \"beginning,\" \"middle,\" and \"end\" sections\n",
    "#     beg_tokens = tokens[:beg_length]\n",
    "#     mid_tokens = tokens[beg_length: beg_length + mid_length]\n",
    "#     end_tokens = tokens[-end_length:]\n",
    "    \n",
    "#     # Append the sections to the transformed_data list\n",
    "#     transformed_data.append([beg_tokens, mid_tokens, end_tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe8629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = normalized_big_df['Domain'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db12116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parts_of_text(content):\n",
    "    words = content.split()  # Splitting the content into words\n",
    "    \n",
    "    # First 50 words\n",
    "    x = \" \".join(words[:50])\n",
    "    \n",
    "    # Middle 300 words\n",
    "    middle_index = len(words) // 2  # Integer division to get middle index\n",
    "    y_start = max(0, middle_index - 150)  # Ensure index is not negative\n",
    "    y_end = middle_index + 150\n",
    "    y = \" \".join(words[y_start:y_end])\n",
    "    \n",
    "    # Last 50 words\n",
    "    z = \" \".join(words[-50:])\n",
    "    \n",
    "    return x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d81d1fc-5b5a-4433-a357-e3a0893aff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.5176093578338623\n",
      "Epoch: 0, Loss:  0.37415120005607605\n",
      "Epoch: 0, Loss:  0.2319658249616623\n",
      "Epoch: 0, Loss:  0.11264555156230927\n",
      "Epoch: 0, Loss:  0.04937262460589409\n",
      "Epoch: 0, Loss:  0.053924813866615295\n",
      "Epoch: 0, Loss:  0.07074303925037384\n",
      "Epoch: 0, Loss:  0.0738525241613388\n",
      "Epoch: 0, Loss:  0.06737595796585083\n",
      "Epoch: 0, Loss:  0.039350394159555435\n",
      "Epoch: 0, Loss:  0.061157990247011185\n",
      "Epoch: 0, Loss:  0.060907959938049316\n",
      "Epoch: 0, Loss:  0.08023978024721146\n",
      "Epoch: 0, Loss:  0.05687671899795532\n",
      "Epoch: 0, Loss:  0.05101495981216431\n",
      "Epoch: 0, Loss:  0.053335197269916534\n",
      "Epoch: 0, Loss:  0.05308596044778824\n",
      "Epoch: 0, Loss:  0.06703256070613861\n",
      "Epoch: 0, Loss:  0.03021511435508728\n",
      "Epoch: 0, Loss:  0.05303748697042465\n",
      "Epoch: 0, Loss:  0.041867610067129135\n",
      "Epoch: 0, Loss:  0.059417903423309326\n",
      "Epoch: 0, Loss:  0.035764455795288086\n",
      "Validation Loss after Epoch 0: 0.04216879084706306\n",
      "Epoch: 1, Loss:  0.04588444158434868\n",
      "Epoch: 1, Loss:  0.050002146512269974\n",
      "Epoch: 1, Loss:  0.05355663225054741\n",
      "Epoch: 1, Loss:  0.039187125861644745\n",
      "Epoch: 1, Loss:  0.0450286790728569\n",
      "Epoch: 1, Loss:  0.057979390025138855\n",
      "Epoch: 1, Loss:  0.03873790055513382\n",
      "Epoch: 1, Loss:  0.0361650288105011\n",
      "Epoch: 1, Loss:  0.029974211007356644\n",
      "Epoch: 1, Loss:  0.0415506511926651\n",
      "Epoch: 1, Loss:  0.03502240031957626\n",
      "Epoch: 1, Loss:  0.028503920882940292\n",
      "Epoch: 1, Loss:  0.03156222775578499\n",
      "Epoch: 1, Loss:  0.03610680252313614\n",
      "Epoch: 1, Loss:  0.03481777012348175\n",
      "Epoch: 1, Loss:  0.05156231299042702\n",
      "Epoch: 1, Loss:  0.027601294219493866\n",
      "Epoch: 1, Loss:  0.037464454770088196\n",
      "Epoch: 1, Loss:  0.028931623324751854\n",
      "Epoch: 1, Loss:  0.03133850917220116\n",
      "Epoch: 1, Loss:  0.034964077174663544\n",
      "Epoch: 1, Loss:  0.04520585015416145\n",
      "Epoch: 1, Loss:  0.03853480890393257\n",
      "Validation Loss after Epoch 1: 0.03026422969996929\n",
      "Epoch: 2, Loss:  0.03399228677153587\n",
      "Epoch: 2, Loss:  0.03563954681158066\n",
      "Epoch: 2, Loss:  0.03540137782692909\n",
      "Epoch: 2, Loss:  0.03171507641673088\n",
      "Epoch: 2, Loss:  0.029550449922680855\n",
      "Epoch: 2, Loss:  0.03753843158483505\n",
      "Epoch: 2, Loss:  0.029866740107536316\n",
      "Epoch: 2, Loss:  0.0511932298541069\n",
      "Epoch: 2, Loss:  0.019312972202897072\n",
      "Epoch: 2, Loss:  0.025523893535137177\n",
      "Epoch: 2, Loss:  0.022344421595335007\n",
      "Epoch: 2, Loss:  0.03162914514541626\n",
      "Epoch: 2, Loss:  0.02672458067536354\n",
      "Epoch: 2, Loss:  0.030581997707486153\n",
      "Epoch: 2, Loss:  0.03177131339907646\n",
      "Epoch: 2, Loss:  0.02107352763414383\n",
      "Epoch: 2, Loss:  0.0398784875869751\n",
      "Epoch: 2, Loss:  0.030748169869184494\n",
      "Epoch: 2, Loss:  0.029486367478966713\n",
      "Epoch: 2, Loss:  0.031426891684532166\n",
      "Epoch: 2, Loss:  0.026406031101942062\n",
      "Epoch: 2, Loss:  0.02191249281167984\n",
      "Epoch: 2, Loss:  0.01644372008740902\n",
      "Validation Loss after Epoch 2: 0.028986011072993277\n",
      "Epoch: 3, Loss:  0.023577779531478882\n",
      "Epoch: 3, Loss:  0.024265091866254807\n",
      "Epoch: 3, Loss:  0.02818228490650654\n",
      "Epoch: 3, Loss:  0.03100438229739666\n",
      "Epoch: 3, Loss:  0.024938145652413368\n",
      "Epoch: 3, Loss:  0.020612958818674088\n",
      "Epoch: 3, Loss:  0.019238431006669998\n",
      "Epoch: 3, Loss:  0.021698730066418648\n",
      "Epoch: 3, Loss:  0.023582156747579575\n",
      "Epoch: 3, Loss:  0.021845584735274315\n",
      "Epoch: 3, Loss:  0.021920669823884964\n",
      "Epoch: 3, Loss:  0.027937045320868492\n",
      "Epoch: 3, Loss:  0.0176857877522707\n",
      "Epoch: 3, Loss:  0.02330178953707218\n",
      "Epoch: 3, Loss:  0.02355111390352249\n",
      "Epoch: 3, Loss:  0.03544842451810837\n",
      "Epoch: 3, Loss:  0.0293587613850832\n",
      "Epoch: 3, Loss:  0.019927194342017174\n",
      "Epoch: 3, Loss:  0.02244226634502411\n",
      "Epoch: 3, Loss:  0.027915721759200096\n",
      "Epoch: 3, Loss:  0.029938220977783203\n",
      "Epoch: 3, Loss:  0.0291318129748106\n",
      "Epoch: 3, Loss:  0.027406025677919388\n",
      "Validation Loss after Epoch 3: 0.027425082772970198\n",
      "Epoch: 4, Loss:  0.019996456801891327\n",
      "Epoch: 4, Loss:  0.02614803984761238\n",
      "Epoch: 4, Loss:  0.022051533684134483\n",
      "Epoch: 4, Loss:  0.02403857931494713\n",
      "Epoch: 4, Loss:  0.017834492027759552\n",
      "Epoch: 4, Loss:  0.02176518924534321\n",
      "Epoch: 4, Loss:  0.022335372865200043\n",
      "Epoch: 4, Loss:  0.013691818341612816\n",
      "Epoch: 4, Loss:  0.02373996376991272\n",
      "Epoch: 4, Loss:  0.02226906828582287\n",
      "Epoch: 4, Loss:  0.02162492461502552\n",
      "Epoch: 4, Loss:  0.02197450026869774\n",
      "Epoch: 4, Loss:  0.014135720208287239\n",
      "Epoch: 4, Loss:  0.02500934526324272\n",
      "Epoch: 4, Loss:  0.021565940231084824\n",
      "Epoch: 4, Loss:  0.02154378592967987\n",
      "Epoch: 4, Loss:  0.03056580200791359\n",
      "Epoch: 4, Loss:  0.019519995898008347\n",
      "Epoch: 4, Loss:  0.021209076046943665\n",
      "Epoch: 4, Loss:  0.01635933294892311\n",
      "Epoch: 4, Loss:  0.03033073991537094\n",
      "Epoch: 4, Loss:  0.022596649825572968\n",
      "Epoch: 4, Loss:  0.014834001660346985\n",
      "Validation Loss after Epoch 4: 0.031189817935228348\n",
      "Epoch: 5, Loss:  0.024730045348405838\n",
      "Epoch: 5, Loss:  0.022251175716519356\n",
      "Epoch: 5, Loss:  0.03094535320997238\n",
      "Epoch: 5, Loss:  0.024237507954239845\n",
      "Epoch: 5, Loss:  0.016356293112039566\n",
      "Epoch: 5, Loss:  0.019230753183364868\n",
      "Epoch: 5, Loss:  0.018625276163220406\n",
      "Epoch: 5, Loss:  0.022694021463394165\n",
      "Epoch: 5, Loss:  0.018443310633301735\n",
      "Epoch: 5, Loss:  0.018949801102280617\n",
      "Epoch: 5, Loss:  0.01897338777780533\n",
      "Epoch: 5, Loss:  0.018796566873788834\n",
      "Epoch: 5, Loss:  0.022826332598924637\n",
      "Epoch: 5, Loss:  0.02072363719344139\n",
      "Epoch: 5, Loss:  0.022900426760315895\n",
      "Epoch: 5, Loss:  0.011229452677071095\n",
      "Epoch: 5, Loss:  0.015146424993872643\n",
      "Epoch: 5, Loss:  0.018832329660654068\n",
      "Epoch: 5, Loss:  0.013749595731496811\n",
      "Epoch: 5, Loss:  0.02413269132375717\n",
      "Epoch: 5, Loss:  0.015384634956717491\n",
      "Epoch: 5, Loss:  0.013391027227044106\n",
      "Epoch: 5, Loss:  0.012726154178380966\n",
      "Validation Loss after Epoch 5: 0.023403131030499937\n",
      "Epoch: 6, Loss:  0.018120944499969482\n",
      "Epoch: 6, Loss:  0.010919399559497833\n",
      "Epoch: 6, Loss:  0.014730612747371197\n",
      "Epoch: 6, Loss:  0.013743136078119278\n",
      "Epoch: 6, Loss:  0.018119508400559425\n",
      "Epoch: 6, Loss:  0.021103251725435257\n",
      "Epoch: 6, Loss:  0.01846877485513687\n",
      "Epoch: 6, Loss:  0.013889732770621777\n",
      "Epoch: 6, Loss:  0.017674367874860764\n",
      "Epoch: 6, Loss:  0.016423299908638\n",
      "Epoch: 6, Loss:  0.019678987562656403\n",
      "Epoch: 6, Loss:  0.015948759391903877\n",
      "Epoch: 6, Loss:  0.012150101363658905\n",
      "Epoch: 6, Loss:  0.022943684831261635\n",
      "Epoch: 6, Loss:  0.017636070027947426\n",
      "Epoch: 6, Loss:  0.010705656372010708\n",
      "Epoch: 6, Loss:  0.0164342001080513\n",
      "Epoch: 6, Loss:  0.022327683866024017\n",
      "Epoch: 6, Loss:  0.015403901226818562\n",
      "Epoch: 6, Loss:  0.010209943167865276\n",
      "Epoch: 6, Loss:  0.016018275171518326\n",
      "Epoch: 6, Loss:  0.021768303588032722\n",
      "Epoch: 6, Loss:  0.01792551577091217\n",
      "Validation Loss after Epoch 6: 0.02381099108606577\n",
      "Epoch: 7, Loss:  0.013823502697050571\n",
      "Epoch: 7, Loss:  0.014655963517725468\n",
      "Epoch: 7, Loss:  0.01431174948811531\n",
      "Epoch: 7, Loss:  0.01197881530970335\n",
      "Epoch: 7, Loss:  0.010435070842504501\n",
      "Epoch: 7, Loss:  0.013396630063652992\n",
      "Epoch: 7, Loss:  0.016825776547193527\n",
      "Epoch: 7, Loss:  0.016552980989217758\n",
      "Epoch: 7, Loss:  0.012943059206008911\n",
      "Epoch: 7, Loss:  0.013415380381047726\n",
      "Epoch: 7, Loss:  0.01438317634165287\n",
      "Epoch: 7, Loss:  0.009844283573329449\n",
      "Epoch: 7, Loss:  0.011493968777358532\n",
      "Epoch: 7, Loss:  0.010324189439415932\n",
      "Epoch: 7, Loss:  0.013311116024851799\n",
      "Epoch: 7, Loss:  0.016512764617800713\n",
      "Epoch: 7, Loss:  0.01663174107670784\n",
      "Epoch: 7, Loss:  0.012542410753667355\n",
      "Epoch: 7, Loss:  0.01547979936003685\n",
      "Epoch: 7, Loss:  0.013969143852591515\n",
      "Epoch: 7, Loss:  0.015905149281024933\n",
      "Epoch: 7, Loss:  0.012892305850982666\n",
      "Epoch: 7, Loss:  0.017255786806344986\n",
      "Validation Loss after Epoch 7: 0.027100593037903308\n",
      "Epoch: 8, Loss:  0.01618771068751812\n",
      "Epoch: 8, Loss:  0.013650238513946533\n",
      "Epoch: 8, Loss:  0.012098344042897224\n",
      "Epoch: 8, Loss:  0.009624245576560497\n",
      "Epoch: 8, Loss:  0.014046072959899902\n",
      "Epoch: 8, Loss:  0.01413748413324356\n",
      "Epoch: 8, Loss:  0.017117977142333984\n",
      "Epoch: 8, Loss:  0.009857295081019402\n",
      "Epoch: 8, Loss:  0.007132877130061388\n",
      "Epoch: 8, Loss:  0.007648455444723368\n",
      "Epoch: 8, Loss:  0.01576695777475834\n",
      "Epoch: 8, Loss:  0.015265011228621006\n",
      "Epoch: 8, Loss:  0.021507102996110916\n",
      "Epoch: 8, Loss:  0.010701773688197136\n",
      "Epoch: 8, Loss:  0.008477257564663887\n",
      "Epoch: 8, Loss:  0.011266253888607025\n",
      "Epoch: 8, Loss:  0.015607527457177639\n",
      "Epoch: 8, Loss:  0.01310553029179573\n",
      "Epoch: 8, Loss:  0.008764560334384441\n",
      "Epoch: 8, Loss:  0.010931361466646194\n",
      "Epoch: 8, Loss:  0.011333191767334938\n",
      "Epoch: 8, Loss:  0.00921577401459217\n",
      "Epoch: 8, Loss:  0.00826363917440176\n",
      "Validation Loss after Epoch 8: 0.024493471905589103\n",
      "Epoch: 9, Loss:  0.01051366701722145\n",
      "Epoch: 9, Loss:  0.0070234015583992004\n",
      "Epoch: 9, Loss:  0.009917149320244789\n",
      "Epoch: 9, Loss:  0.007183747366070747\n",
      "Epoch: 9, Loss:  0.00841711089015007\n",
      "Epoch: 9, Loss:  0.00937610026448965\n",
      "Epoch: 9, Loss:  0.00920974463224411\n",
      "Epoch: 9, Loss:  0.008649662137031555\n",
      "Epoch: 9, Loss:  0.012023739516735077\n",
      "Epoch: 9, Loss:  0.010518998838961124\n",
      "Epoch: 9, Loss:  0.008037328720092773\n",
      "Epoch: 9, Loss:  0.008484347723424435\n",
      "Epoch: 9, Loss:  0.006228697020560503\n",
      "Epoch: 9, Loss:  0.009836921468377113\n",
      "Epoch: 9, Loss:  0.012234622612595558\n",
      "Epoch: 9, Loss:  0.0077046677470207214\n",
      "Epoch: 9, Loss:  0.012047669850289822\n",
      "Epoch: 9, Loss:  0.008221400901675224\n",
      "Epoch: 9, Loss:  0.01104777492582798\n",
      "Epoch: 9, Loss:  0.010141929611563683\n",
      "Epoch: 9, Loss:  0.011860729195177555\n",
      "Epoch: 9, Loss:  0.011551973409950733\n",
      "Epoch: 9, Loss:  0.008057043887674809\n",
      "Validation Loss after Epoch 9: 0.025286186300218106\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Initialize DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1).to(device)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        content_len = len(text.split())\n",
    "\n",
    "        first_50_words, middle_300_words, last_50_words = get_parts_of_text(text)\n",
    "\n",
    "        test_example = f\"\"\"\n",
    "        Total Content Length:{content_len}\n",
    "\n",
    "        Text Snippets:\n",
    "        First 50 words of content:\n",
    "        {first_50_words}\n",
    "        Middle 300 words of content:\n",
    "        {middle_300_words}\n",
    "        Last 50 words of content:\n",
    "        {last_50_words}\n",
    "        \"\"\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            test_example,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Splitting data\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_len=512)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_len=512)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Loss & Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "n_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch['text'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].view(-1, 1).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
    "        loss = loss_fn(outputs.to(device), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss:  {loss.item()}\")\n",
    "    \n",
    "    # Validation Loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            input_ids = batch['text'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].view(-1, 1).to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
    "            loss = loss_fn(outputs.to(device), labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Validation Loss after Epoch {epoch}: {val_loss}\")\n",
    "    \n",
    "    # Check for Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce010699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"C:/Users/William Zhang/OneDrive/Desktop/Search_Sense_Model\"\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1073c08d-3b54-4bff-88ba-0fb29d0ba5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to make predictions\n",
    "def make_prediction(text, model, tokenizer):\n",
    "\n",
    "    content_len = len(text.split())\n",
    "    first_50_words, middle_300_words, last_50_words = get_parts_of_text(text)\n",
    "\n",
    "    text_final_input = f\"\"\"\n",
    "    Total Content Length:{content_len}\n",
    "\n",
    "    Text Snippets:\n",
    "    First 50 words of content:\n",
    "    {first_50_words}\n",
    "    Middle 300 words of content:\n",
    "    {middle_300_words}\n",
    "    Last 50 words of content:\n",
    "    {last_50_words}\n",
    "    \"\"\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text_final_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Set model to evaluation mode and make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    # Convert output to label\n",
    "    predicted_score = output.item()\n",
    "    \n",
    "    return predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dced058-830e-4bbe-b0e1-fea1ae8d2a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\William\n",
      "[nltk_data]     Zhang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\William\n",
      "[nltk_data]     Zhang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\William\n",
      "[nltk_data]     Zhang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import WebScrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9606bbc3-6680-4b70-8004-b8253701e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping...:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to retrieve content for:Error: Failed to retrieve content for: https://www.seldon.io/supervised-vs-unsupervised-learning-explained#:~:text=Supervised%20machine%20learning%20is%20generally,the%20need%20for%20labelled%20data.\n",
      "Exception: HTTPSConnectionPool(host='www.seldon.io', port=443): Max retries exceeded with url: /supervised-vs-unsupervised-learning-explained (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF03182CE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      " https://www.seldon.io/supervised-vs-unsupervised-learning-explained#:~:text=Supervised%20machine%20learning%20is%20generally,the%20need%20for%20labelled%20data.\n",
      "Exception: HTTPSConnectionPool(host='www.seldon.io', port=443): Max retries exceeded with url: /supervised-vs-unsupervised-learning-explained (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF03183940>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error: Failed to retrieve content for: https://www.simplilearn.com/tutorials/machine-learning-tutorial/supervised-and-unsupervised-learning\n",
      "Exception: HTTPSConnectionPool(host='www.simplilearn.com', port=443): Max retries exceeded with url: /tutorials/machine-learning-tutorial/supervised-and-unsupervised-learning (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF03182080>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error: Failed to retrieve content for:Error: Failed to retrieve content for: https://blogs.oracle.com/ai-and-datascience/post/supervised-vs-unsupervised-machine-learning\n",
      "Exception: HTTPSConnectionPool(host='blogs.oracle.com', port=443): Max retries exceeded with url: /ai-and-datascience/post/supervised-vs-unsupervised-machine-learning (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF030F3760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      " https://rapidminer.com/blog/supervised-vs-unsupervised-machine-learning/\n",
      "Exception: HTTPSConnectionPool(host='rapidminer.com', port=443): Max retries exceeded with url: /blog/supervised-vs-unsupervised-machine-learning/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF030F2EF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error: Failed to retrieve content for: https://www.enjoyalgorithms.com/blogs/supervised-unsupervised-and-semisupervised-learning/\n",
      "Exception: HTTPSConnectionPool(host='www.enjoyalgorithms.com', port=443): Max retries exceeded with url: /blogs/supervised-unsupervised-and-semisupervised-learning/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF049D8F40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error: Failed to retrieve content for: https://www.itmagination.com/blog/differences-between-supervised-unsupervised-machine-learning\n",
      "Exception: HTTPSConnectionPool(host='www.itmagination.com', port=443): Max retries exceeded with url: /blog/differences-between-supervised-unsupervised-machine-learning (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF030D7B80>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error: Failed to retrieve content for: https://cloudinfrastructureservices.co.uk/supervised-learning-vs-unsupervised-learning/\n",
      "Exception: HTTPSConnectionPool(host='cloudinfrastructureservices.co.uk', port=443): Max retries exceeded with url: /supervised-learning-vs-unsupervised-learning/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF03181A20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error: Failed to retrieve content for: https://domino.ai/blog/supervised-vs-unsupervised-learning\n",
      "Exception: HTTPSConnectionPool(host='domino.ai', port=443): Max retries exceeded with url: /blog/supervised-vs-unsupervised-learning (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF04BF1DB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error: Failed to retrieve content for: https://www.exxactcorp.com/blog/Deep-Learning/supervised-vs-unsupervised-machine-learning\n",
      "Exception: HTTPSConnectionPool(host='www.exxactcorp.com', port=443): Max retries exceeded with url: /blog/Deep-Learning/supervised-vs-unsupervised-machine-learning (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002AF04DD3D00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbed search results\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "hi = WebScrape.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0afd56b-3018-47aa-b603-100b42f2c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = hi.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a58946-8f8a-4ff5-a237-b6aec774cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove yt videos\n",
    "preprocess_df = preprocess_df[~preprocess_df['Domain'].str.contains('youtube')]\n",
    "#remove null or empty values\n",
    "preprocess_df.dropna(subset=[\"Text\"], inplace=True)\n",
    "#remove entries with less than 25 words\n",
    "preprocess_df = preprocess_df[preprocess_df[\"Text\"].apply(lambda x: len(x.split()) >= 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd6e825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.codecademy.com/article/machine-learning-supervised-vs-unsupervised'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_df[\"URL\"][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e94122-7fd2-4a33-90e9-aad932b43d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.alteryx.com/glossary/supervised-vs-unsupervised-learning'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_df[\"URL\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3ace93e-1ac0-49e4-8f4c-938bbf8a2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = make_prediction(preprocess_df[\"Text\"][18], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "879e784f-cd66-45c5-9023-6b4a5ad5334a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.348219096660614"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b6bd562c-7769-4f8a-beef-74e32cc6159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com/advice/0/what-differences-similarities-between-supervised'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_df[\"URL\"][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5b5c4cfb-5d3d-424b-80a3-b9f7488f6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = make_prediction(preprocess_df[\"Text\"][29], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ce969b1e-04b1-4685-a69d-d4f33964e03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222419261932373"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "82bce9e5-68c6-48a2-843d-f31f7e42011c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seldon</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.seldon.io/supervised-vs-unsupervis...</td>\n",
       "      <td>Supervised vs Unsupervised Learning Explained ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seldon</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.seldon.io/supervised-vs-unsupervis...</td>\n",
       "      <td>Supervised vs Unsupervised Learning Explained ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geeksforgeeks</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.geeksforgeeks.org/supervised-unsup...</td>\n",
       "      <td>Supervised and Unsupervised learning - Geeksfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simplilearn</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.simplilearn.com/tutorials/machine-...</td>\n",
       "      <td>Supervised and Unsupervised Learning in (Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alteryx</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.alteryx.com/glossary/supervised-vs...</td>\n",
       "      <td>Supervised vs. Unsupervised Learning; Which Is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v7labs</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.v7labs.com/blog/supervised-vs-unsu...</td>\n",
       "      <td>Supervised vs. Unsupervised Learning [Differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cs.stackexchange</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://cs.stackexchange.com/questions/2907/wh...</td>\n",
       "      <td>data mining - What exactly is the difference b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>techtarget</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.techtarget.com/searchenterpriseai/...</td>\n",
       "      <td>Comparing Supervised vs. Unsupervised Learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>towardsdatascience</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://towardsdatascience.com/supervised-vs-u...</td>\n",
       "      <td>Supervised vs. Unsupervised Learning | by Devi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tutorialforbeginner</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://tutorialforbeginner.com/supervised-vs-...</td>\n",
       "      <td>Supervised vs Unsupervised Machine Learning | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>arshren.medium</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://arshren.medium.com/supervised-unsuperv...</td>\n",
       "      <td>Supervised, Unsupervised, and Reinforcement Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>labelyourdata</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://labelyourdata.com/articles/supervised-...</td>\n",
       "      <td>Machine Learning 101: Supervised vs. Unsupervi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lifewire</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.lifewire.com/supervised-vs-unsuper...</td>\n",
       "      <td>Supervised vs. Unsupervised Learning: What's t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>freecodecamp</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.freecodecamp.org/news/supervised-v...</td>\n",
       "      <td>Supervised vs Unsupervised Learning â€“ What's t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>educative</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.educative.io/answers/supervised-vs...</td>\n",
       "      <td>Supervised vs. unsupervised vs. reinforcement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>makeuseof</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.makeuseof.com/supervised-vs-unsupe...</td>\n",
       "      <td>Supervised vs. Unsupervised Learning: What's t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ibm</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.ibm.com/blog/supervised-vs-unsuper...</td>\n",
       "      <td>Supervised vs. Unsupervised Learning: Whatâ€™s t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>intellipaat</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://intellipaat.com/blog/supervised-learni...</td>\n",
       "      <td>Difference Between Supervised and Unsupervised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>scribbr</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.scribbr.com/ai-tools/supervised-vs...</td>\n",
       "      <td>Supervised vs. Unsupervised Learning: Key Diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>quora</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.quora.com/What-is-the-difference-b...</td>\n",
       "      <td>What is the difference between supervised and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>linkedin</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.linkedin.com/advice/0/what-differe...</td>\n",
       "      <td>Supervised vs Unsupervised Statistical Learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>enjoyalgorithms</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.enjoyalgorithms.com/blogs/supervis...</td>\n",
       "      <td>Supervised, Unsupervised and Semi-supervised L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>stratascratch</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.stratascratch.com/blog/supervised-...</td>\n",
       "      <td>difference is that with HDBSCAN, we donâ€™t need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>codecademy</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.codecademy.com/article/machine-lea...</td>\n",
       "      <td>Supervised vs. Unsupervised | CodecademyLoadin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>labellerr</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.labellerr.com/blog/supervised-vs-u...</td>\n",
       "      <td>Mastering of Supervised and Unsupervised Learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>viso</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://viso.ai/deep-learning/supervised-vs-un...</td>\n",
       "      <td>Supervised vs Unsupervised Learning for Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>moveworks</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.moveworks.com/insights/supervised-...</td>\n",
       "      <td>Supervised vs Unsupervised Learning Explained ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>aitude</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.aitude.com/supervised-vs-unsupervi...</td>\n",
       "      <td>Supervised vs Unsupervised vs Reinforcement - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>itmagination</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.itmagination.com/blog/differences-...</td>\n",
       "      <td>Whatâ€™s Supervised and Unsupervised Machine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bmc</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://www.bmc.com/blogs/supervised-vs-unsupe...</td>\n",
       "      <td>Supervised, Unsupervised &amp; Other Machine Learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>pythonprogramminglanguage</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "      <td>https://pythonprogramminglanguage.com/what-is-...</td>\n",
       "      <td>Supervised and Unsupervised Learning compared ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Domain  \\\n",
       "1                      seldon   \n",
       "2                      seldon   \n",
       "3               geeksforgeeks   \n",
       "4                 simplilearn   \n",
       "6                     alteryx   \n",
       "7                      v7labs   \n",
       "8            cs.stackexchange   \n",
       "10                 techtarget   \n",
       "11         towardsdatascience   \n",
       "16        tutorialforbeginner   \n",
       "17             arshren.medium   \n",
       "18              labelyourdata   \n",
       "20                   lifewire   \n",
       "21               freecodecamp   \n",
       "22                  educative   \n",
       "23                  makeuseof   \n",
       "24                        ibm   \n",
       "26                intellipaat   \n",
       "27                    scribbr   \n",
       "28                      quora   \n",
       "29                   linkedin   \n",
       "31            enjoyalgorithms   \n",
       "32              stratascratch   \n",
       "33                 codecademy   \n",
       "36                  labellerr   \n",
       "37                       viso   \n",
       "38                  moveworks   \n",
       "39                     aitude   \n",
       "40               itmagination   \n",
       "43                        bmc   \n",
       "44  pythonprogramminglanguage   \n",
       "\n",
       "                                                Title  \\\n",
       "1   Supervised and Unsupervised Learning compared ...   \n",
       "2   Supervised and Unsupervised Learning compared ...   \n",
       "3   Supervised and Unsupervised Learning compared ...   \n",
       "4   Supervised and Unsupervised Learning compared ...   \n",
       "6   Supervised and Unsupervised Learning compared ...   \n",
       "7   Supervised and Unsupervised Learning compared ...   \n",
       "8   Supervised and Unsupervised Learning compared ...   \n",
       "10  Supervised and Unsupervised Learning compared ...   \n",
       "11  Supervised and Unsupervised Learning compared ...   \n",
       "16  Supervised and Unsupervised Learning compared ...   \n",
       "17  Supervised and Unsupervised Learning compared ...   \n",
       "18  Supervised and Unsupervised Learning compared ...   \n",
       "20  Supervised and Unsupervised Learning compared ...   \n",
       "21  Supervised and Unsupervised Learning compared ...   \n",
       "22  Supervised and Unsupervised Learning compared ...   \n",
       "23  Supervised and Unsupervised Learning compared ...   \n",
       "24  Supervised and Unsupervised Learning compared ...   \n",
       "26  Supervised and Unsupervised Learning compared ...   \n",
       "27  Supervised and Unsupervised Learning compared ...   \n",
       "28  Supervised and Unsupervised Learning compared ...   \n",
       "29  Supervised and Unsupervised Learning compared ...   \n",
       "31  Supervised and Unsupervised Learning compared ...   \n",
       "32  Supervised and Unsupervised Learning compared ...   \n",
       "33  Supervised and Unsupervised Learning compared ...   \n",
       "36  Supervised and Unsupervised Learning compared ...   \n",
       "37  Supervised and Unsupervised Learning compared ...   \n",
       "38  Supervised and Unsupervised Learning compared ...   \n",
       "39  Supervised and Unsupervised Learning compared ...   \n",
       "40  Supervised and Unsupervised Learning compared ...   \n",
       "43  Supervised and Unsupervised Learning compared ...   \n",
       "44  Supervised and Unsupervised Learning compared ...   \n",
       "\n",
       "                                                  URL  \\\n",
       "1   https://www.seldon.io/supervised-vs-unsupervis...   \n",
       "2   https://www.seldon.io/supervised-vs-unsupervis...   \n",
       "3   https://www.geeksforgeeks.org/supervised-unsup...   \n",
       "4   https://www.simplilearn.com/tutorials/machine-...   \n",
       "6   https://www.alteryx.com/glossary/supervised-vs...   \n",
       "7   https://www.v7labs.com/blog/supervised-vs-unsu...   \n",
       "8   https://cs.stackexchange.com/questions/2907/wh...   \n",
       "10  https://www.techtarget.com/searchenterpriseai/...   \n",
       "11  https://towardsdatascience.com/supervised-vs-u...   \n",
       "16  https://tutorialforbeginner.com/supervised-vs-...   \n",
       "17  https://arshren.medium.com/supervised-unsuperv...   \n",
       "18  https://labelyourdata.com/articles/supervised-...   \n",
       "20  https://www.lifewire.com/supervised-vs-unsuper...   \n",
       "21  https://www.freecodecamp.org/news/supervised-v...   \n",
       "22  https://www.educative.io/answers/supervised-vs...   \n",
       "23  https://www.makeuseof.com/supervised-vs-unsupe...   \n",
       "24  https://www.ibm.com/blog/supervised-vs-unsuper...   \n",
       "26  https://intellipaat.com/blog/supervised-learni...   \n",
       "27  https://www.scribbr.com/ai-tools/supervised-vs...   \n",
       "28  https://www.quora.com/What-is-the-difference-b...   \n",
       "29  https://www.linkedin.com/advice/0/what-differe...   \n",
       "31  https://www.enjoyalgorithms.com/blogs/supervis...   \n",
       "32  https://www.stratascratch.com/blog/supervised-...   \n",
       "33  https://www.codecademy.com/article/machine-lea...   \n",
       "36  https://www.labellerr.com/blog/supervised-vs-u...   \n",
       "37  https://viso.ai/deep-learning/supervised-vs-un...   \n",
       "38  https://www.moveworks.com/insights/supervised-...   \n",
       "39  https://www.aitude.com/supervised-vs-unsupervi...   \n",
       "40  https://www.itmagination.com/blog/differences-...   \n",
       "43  https://www.bmc.com/blogs/supervised-vs-unsupe...   \n",
       "44  https://pythonprogramminglanguage.com/what-is-...   \n",
       "\n",
       "                                                 Text  \n",
       "1   Supervised vs Unsupervised Learning Explained ...  \n",
       "2   Supervised vs Unsupervised Learning Explained ...  \n",
       "3   Supervised and Unsupervised learning - Geeksfo...  \n",
       "4   Supervised and Unsupervised Learning in (Machi...  \n",
       "6   Supervised vs. Unsupervised Learning; Which Is...  \n",
       "7   Supervised vs. Unsupervised Learning [Differen...  \n",
       "8   data mining - What exactly is the difference b...  \n",
       "10  Comparing Supervised vs. Unsupervised Learning...  \n",
       "11  Supervised vs. Unsupervised Learning | by Devi...  \n",
       "16  Supervised vs Unsupervised Machine Learning | ...  \n",
       "17  Supervised, Unsupervised, and Reinforcement Le...  \n",
       "18  Machine Learning 101: Supervised vs. Unsupervi...  \n",
       "20  Supervised vs. Unsupervised Learning: What's t...  \n",
       "21  Supervised vs Unsupervised Learning â€“ What's t...  \n",
       "22  Supervised vs. unsupervised vs. reinforcement ...  \n",
       "23  Supervised vs. Unsupervised Learning: What's t...  \n",
       "24  Supervised vs. Unsupervised Learning: Whatâ€™s t...  \n",
       "26  Difference Between Supervised and Unsupervised...  \n",
       "27  Supervised vs. Unsupervised Learning: Key Diff...  \n",
       "28  What is the difference between supervised and ...  \n",
       "29  Supervised vs Unsupervised Statistical Learnin...  \n",
       "31  Supervised, Unsupervised and Semi-supervised L...  \n",
       "32  difference is that with HDBSCAN, we donâ€™t need...  \n",
       "33  Supervised vs. Unsupervised | CodecademyLoadin...  \n",
       "36  Mastering of Supervised and Unsupervised Learn...  \n",
       "37  Supervised vs Unsupervised Learning for Comput...  \n",
       "38  Supervised vs Unsupervised Learning Explained ...  \n",
       "39  Supervised vs Unsupervised vs Reinforcement - ...  \n",
       "40  Whatâ€™s Supervised and Unsupervised Machine Lea...  \n",
       "43  Supervised, Unsupervised & Other Machine Learn...  \n",
       "44  Supervised and Unsupervised Learning compared ...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41636749-915d-4580-a3f5-ebe8d1931b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
